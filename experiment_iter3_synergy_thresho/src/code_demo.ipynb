{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ilff3myesi",
   "metadata": {},
   "source": [
    "# Synergy Threshold Sensitivity & Adaptive Thresholding for SG-FIGS\n",
    "\n",
    "This notebook demonstrates the **SG-FIGS** (Synergy-Guided Fast Interpretable Greedy-tree Sums) method, which uses feature synergy information to guide oblique splits in interpretable tree models.\n",
    "\n",
    "**What this experiment does:**\n",
    "1. Loads pre-computed experiment results (balanced accuracy, AUC, interpretability scores) across multiple datasets\n",
    "2. Compares **SG-FIGS** against baselines: Standard FIGS (axis-aligned), RO-FIGS (random oblique), and GBDT\n",
    "3. Evaluates synergy threshold percentiles (50th, 75th, 90th) and their impact on model performance\n",
    "4. Analyzes correlations between synergy distribution properties and optimal threshold selection\n",
    "5. Computes aggregate statistics and determines the best universal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "au87ms6txh",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q --force-reinstall numpy==1.26.4 scipy==1.15.3 scikit-learn==1.7.2 networkx==3.4.2 matplotlib==3.10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i8byc4kog6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mutual_info_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "el4cvs3w37t",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-fb8249-synergy-guided-oblique-splits-using-part/main/experiment_iter3_sg_figs_thresh/demo/mini_demo_data.json\"\n",
    "import json, os\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        import urllib.request\n",
    "        with urllib.request.urlopen(GITHUB_DATA_URL) as response:\n",
    "            return json.loads(response.read().decode())\n",
    "    except Exception: pass\n",
    "    if os.path.exists(\"mini_demo_data.json\"):\n",
    "        with open(\"mini_demo_data.json\") as f: return json.load(f)\n",
    "    raise FileNotFoundError(\"Could not load mini_demo_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dwd0rxvevv",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "print(f\"Loaded {len(data['datasets'])} datasets\")\n",
    "for ds in data[\"datasets\"]:\n",
    "    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66qes59dhkp",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Tunable parameters for the experiment. Threshold percentiles control how aggressively the synergy graph is pruned, max splits control tree complexity, and N_FOLDS sets cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6fjqoh4ul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tunable parameters ---\n",
    "# These filter the pre-computed results loaded from mini_demo_data.json\n",
    "THRESHOLD_PERCENTILES = [50, 75, 90]\n",
    "MAX_SPLITS_VALUES = [5, 10, 15]  # Original values\n",
    "N_FOLDS = 5  # Original value\n",
    "RANDOM_SEED = 42\n",
    "ADAPTIVE_CANDIDATES = [50, 60, 70, 80, 90]  # Original values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dsrpxvkx26",
   "metadata": {},
   "source": [
    "## Parse Datasets from Loaded Data\n",
    "\n",
    "Extract feature matrices X, labels y, and fold assignments from the JSON experiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jgfyg051scq",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datasets(data):\n",
    "    \"\"\"Parse datasets from mini_demo_data.json format into numpy arrays.\"\"\"\n",
    "    datasets = {}\n",
    "    for ds_entry in data[\"datasets\"]:\n",
    "        name = ds_entry[\"dataset\"]\n",
    "        examples = ds_entry[\"examples\"]\n",
    "        # Each example has input (JSON with features) and output (JSON with metrics)\n",
    "        # We parse input to get experiment config, output to get results\n",
    "        parsed_examples = []\n",
    "        for ex in examples:\n",
    "            inp = json.loads(ex[\"input\"])\n",
    "            out = json.loads(ex[\"output\"])\n",
    "            parsed_examples.append({\n",
    "                \"input\": inp,\n",
    "                \"output\": out,\n",
    "                \"metadata_fold\": ex[\"metadata_fold\"],\n",
    "                \"metadata_threshold_percentile\": ex[\"metadata_threshold_percentile\"],\n",
    "                \"metadata_max_splits\": ex[\"metadata_max_splits\"],\n",
    "            })\n",
    "        datasets[name] = {\n",
    "            \"examples\": parsed_examples,\n",
    "            \"n_features\": parsed_examples[0][\"input\"][\"n_features\"],\n",
    "        }\n",
    "    return datasets\n",
    "\n",
    "all_datasets = parse_datasets(data)\n",
    "print(f\"Parsed {len(all_datasets)} datasets:\")\n",
    "for name, info in all_datasets.items():\n",
    "    print(f\"  {name}: {len(info['examples'])} examples, {info['n_features']} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sfodajsk7eb",
   "metadata": {},
   "source": [
    "## Per-Dataset Analysis\n",
    "\n",
    "Aggregate results by threshold percentile for each dataset. Compute mean balanced accuracy and AUC across folds/splits for each threshold setting, identify optimal thresholds, and measure improvement over the default 75th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1apk1offvg",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "per_dataset_analysis = {}\n",
    "dataset_names = sorted(all_datasets.keys())\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds = all_datasets[ds_name]\n",
    "    examples = ds[\"examples\"]\n",
    "\n",
    "    # Group by threshold percentile\n",
    "    threshold_results = {}\n",
    "    for pct in THRESHOLD_PERCENTILES:\n",
    "        pct_examples = [ex for ex in examples if ex[\"metadata_threshold_percentile\"] == pct]\n",
    "        if not pct_examples:\n",
    "            threshold_results[str(pct)] = {\"mean_balanced_acc\": 0.5, \"std_balanced_acc\": 0.0, \"mean_auc\": 0.5}\n",
    "            continue\n",
    "\n",
    "        sg_accs = [ex[\"output\"][\"sg_figs_balanced_acc\"] for ex in pct_examples]\n",
    "        sg_aucs = [ex[\"output\"][\"sg_figs_auc\"] for ex in pct_examples]\n",
    "        figs_accs = [ex[\"output\"][\"figs_balanced_acc\"] for ex in pct_examples]\n",
    "        rofigs_accs = [ex[\"output\"][\"rofigs_balanced_acc\"] for ex in pct_examples]\n",
    "        gbdt_accs = [ex[\"output\"][\"gbdt_balanced_acc\"] for ex in pct_examples]\n",
    "\n",
    "        threshold_results[str(pct)] = {\n",
    "            \"mean_balanced_acc\": round(float(np.mean(sg_accs)), 6),\n",
    "            \"std_balanced_acc\": round(float(np.std(sg_accs)), 6),\n",
    "            \"mean_auc\": round(float(np.mean(sg_aucs)), 6),\n",
    "            \"mean_figs_acc\": round(float(np.mean(figs_accs)), 6),\n",
    "            \"mean_rofigs_acc\": round(float(np.mean(rofigs_accs)), 6),\n",
    "            \"mean_gbdt_acc\": round(float(np.mean(gbdt_accs)), 6),\n",
    "        }\n",
    "\n",
    "    # Find optimal threshold\n",
    "    best_pct = max(THRESHOLD_PERCENTILES, key=lambda p: threshold_results[str(p)][\"mean_balanced_acc\"])\n",
    "    best_acc = threshold_results[str(best_pct)][\"mean_balanced_acc\"]\n",
    "    fixed_75 = threshold_results.get(\"75\", {}).get(\"mean_balanced_acc\", 0.5)\n",
    "\n",
    "    per_dataset_analysis[ds_name] = {\n",
    "        \"threshold_results\": threshold_results,\n",
    "        \"optimal_threshold\": best_pct,\n",
    "        \"optimal_acc\": best_acc,\n",
    "        \"fixed_75_acc\": fixed_75,\n",
    "        \"improvement\": round(best_acc - fixed_75, 6),\n",
    "        \"n_features\": ds[\"n_features\"],\n",
    "    }\n",
    "\n",
    "print(f\"Analysis completed in {time.time() - start_time:.2f}s\")\n",
    "print(f\"\\nPer-dataset optimal thresholds:\")\n",
    "for ds_name in dataset_names:\n",
    "    a = per_dataset_analysis[ds_name]\n",
    "    print(f\"  {ds_name:45s} | optimal: {a['optimal_threshold']:3d}th pct | \"\n",
    "          f\"acc: {a['optimal_acc']:.4f} | improvement: {a['improvement']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53mdtf3qkr",
   "metadata": {},
   "source": [
    "## Aggregate Statistics & Correlation Analysis\n",
    "\n",
    "Compute overall statistics across all datasets: best universal threshold, SG-FIGS vs baselines mean differences, and Spearman correlations between dataset properties and optimal thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hicwltubw5i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate statistics ---\n",
    "improvements = [per_dataset_analysis[d][\"improvement\"] for d in dataset_names]\n",
    "n_improved = sum(1 for imp in improvements if imp > 0.001)\n",
    "\n",
    "# Best universal threshold (highest mean accuracy across datasets)\n",
    "universal_accs = {}\n",
    "for pct in THRESHOLD_PERCENTILES:\n",
    "    accs = [per_dataset_analysis[d][\"threshold_results\"][str(pct)][\"mean_balanced_acc\"]\n",
    "            for d in dataset_names]\n",
    "    universal_accs[pct] = float(np.mean(accs))\n",
    "best_universal = max(universal_accs, key=universal_accs.get)\n",
    "\n",
    "# SG-FIGS vs baselines (using 75th percentile as reference)\n",
    "sg_vs_figs = []\n",
    "sg_vs_rofigs = []\n",
    "sg_vs_gbdt = []\n",
    "for ds_name in dataset_names:\n",
    "    tr = per_dataset_analysis[ds_name][\"threshold_results\"].get(\"75\", {})\n",
    "    if \"mean_figs_acc\" in tr:\n",
    "        sg_vs_figs.append(tr[\"mean_balanced_acc\"] - tr[\"mean_figs_acc\"])\n",
    "        sg_vs_rofigs.append(tr[\"mean_balanced_acc\"] - tr[\"mean_rofigs_acc\"])\n",
    "        sg_vs_gbdt.append(tr[\"mean_balanced_acc\"] - tr[\"mean_gbdt_acc\"])\n",
    "\n",
    "aggregate = {\n",
    "    \"mean_improvement_from_tuning\": round(float(np.mean(improvements)), 6),\n",
    "    \"n_datasets_improved\": n_improved,\n",
    "    \"best_universal_threshold\": int(best_universal),\n",
    "    \"sg_figs_vs_figs_mean_diff\": round(float(np.mean(sg_vs_figs)), 6) if sg_vs_figs else 0.0,\n",
    "    \"sg_figs_vs_rofigs_mean_diff\": round(float(np.mean(sg_vs_rofigs)), 6) if sg_vs_rofigs else 0.0,\n",
    "    \"sg_figs_vs_gbdt_mean_diff\": round(float(np.mean(sg_vs_gbdt)), 6) if sg_vs_gbdt else 0.0,\n",
    "}\n",
    "\n",
    "# Correlation analysis\n",
    "opt_pcts = [per_dataset_analysis[d][\"optimal_threshold\"] for d in dataset_names]\n",
    "n_feats = [per_dataset_analysis[d][\"n_features\"] for d in dataset_names]\n",
    "\n",
    "def safe_spearman(a, b):\n",
    "    try:\n",
    "        rho, pval = stats.spearmanr(a, b)\n",
    "        return {\"rho\": round(float(rho), 4) if not np.isnan(rho) else 0.0,\n",
    "                \"p_value\": round(float(pval), 4) if not np.isnan(pval) else 1.0}\n",
    "    except Exception:\n",
    "        return {\"rho\": 0.0, \"p_value\": 1.0}\n",
    "\n",
    "correlation_analysis = {\n",
    "    \"optimal_percentile_vs_n_features\": safe_spearman(opt_pcts, n_feats),\n",
    "    \"improvement_vs_n_features\": safe_spearman(improvements, n_feats),\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AGGREGATE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Best universal threshold: {aggregate['best_universal_threshold']}th percentile\")\n",
    "print(f\"  Mean improvement from tuning: {aggregate['mean_improvement_from_tuning']:.4f}\")\n",
    "print(f\"  Datasets improved: {aggregate['n_datasets_improved']}/{len(dataset_names)}\")\n",
    "print(f\"  SG-FIGS vs FIGS: {aggregate['sg_figs_vs_figs_mean_diff']:+.4f}\")\n",
    "print(f\"  SG-FIGS vs RO-FIGS: {aggregate['sg_figs_vs_rofigs_mean_diff']:+.4f}\")\n",
    "print(f\"  SG-FIGS vs GBDT: {aggregate['sg_figs_vs_gbdt_mean_diff']:+.4f}\")\n",
    "print(f\"\\nCorrelation analysis:\")\n",
    "for key, val in correlation_analysis.items():\n",
    "    print(f\"  {key}: rho={val['rho']:.4f}, p={val['p_value']:.4f}\")\n",
    "print(f\"\\nUniversal threshold accuracies:\")\n",
    "for pct, acc in sorted(universal_accs.items()):\n",
    "    marker = \" <-- best\" if pct == best_universal else \"\"\n",
    "    print(f\"  {pct}th percentile: {acc:.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zgyk3obgzqk",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "**Left**: SG-FIGS balanced accuracy across synergy threshold percentiles for each dataset.\n",
    "**Right**: Method comparison \u2014 SG-FIGS vs FIGS, RO-FIGS, and GBDT baselines at the 75th percentile threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lnw7o4dkjyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Plot 1: Threshold sensitivity per dataset ---\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(dataset_names)))\n",
    "for idx, ds_name in enumerate(dataset_names):\n",
    "    accs = [per_dataset_analysis[ds_name][\"threshold_results\"][str(p)][\"mean_balanced_acc\"]\n",
    "            for p in THRESHOLD_PERCENTILES]\n",
    "    short_name = ds_name[:15] + \"...\" if len(ds_name) > 15 else ds_name\n",
    "    ax1.plot(THRESHOLD_PERCENTILES, accs, 'o-', color=colors[idx], label=short_name, linewidth=1.5, markersize=5)\n",
    "ax1.set_xlabel(\"Synergy Threshold Percentile\")\n",
    "ax1.set_ylabel(\"Mean Balanced Accuracy\")\n",
    "ax1.set_title(\"Threshold Sensitivity by Dataset\")\n",
    "ax1.legend(fontsize=7, loc=\"best\")\n",
    "ax1.set_xticks(THRESHOLD_PERCENTILES)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 2: Method comparison at 75th percentile ---\n",
    "ax2 = axes[1]\n",
    "methods = [\"SG-FIGS\", \"FIGS\", \"RO-FIGS\", \"GBDT\"]\n",
    "method_keys = [\"mean_balanced_acc\", \"mean_figs_acc\", \"mean_rofigs_acc\", \"mean_gbdt_acc\"]\n",
    "x = np.arange(len(dataset_names))\n",
    "width = 0.18\n",
    "\n",
    "for i, (method, key) in enumerate(zip(methods, method_keys)):\n",
    "    vals = []\n",
    "    for ds_name in dataset_names:\n",
    "        tr = per_dataset_analysis[ds_name][\"threshold_results\"].get(\"75\", {})\n",
    "        vals.append(tr.get(key, 0.5))\n",
    "    ax2.bar(x + i * width, vals, width, label=method, alpha=0.85)\n",
    "\n",
    "short_names = [n[:8] + \"..\" if len(n) > 10 else n for n in dataset_names]\n",
    "ax2.set_xticks(x + width * 1.5)\n",
    "ax2.set_xticklabels(short_names, rotation=45, ha=\"right\", fontsize=7)\n",
    "ax2.set_ylabel(\"Balanced Accuracy\")\n",
    "ax2.set_title(\"Method Comparison (75th Percentile)\")\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal notebook runtime: {time.time() - start_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
